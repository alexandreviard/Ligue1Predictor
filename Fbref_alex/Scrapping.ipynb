{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Scrapping.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m request1 \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url_ligue1, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(request1\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m url_equipes \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mtable.stats_table\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m url_equipes \u001b[39m=\u001b[39m [equipes\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m equipes \u001b[39min\u001b[39;00m url_equipes]\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m url_equipes \u001b[39m=\u001b[39m [equipes \u001b[39mfor\u001b[39;00m equipes \u001b[39min\u001b[39;00m url_equipes \u001b[39mif\u001b[39;00m equipes \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msquads\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m equipes]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# L'URL que vous scrapez\n",
    "url_ligue1 = \"https://fbref.com/en/comps/13/Ligue-1-Stats\"\n",
    "left_on = [\"Time\", \"Comp\", \"Round\", \"Day\", \"Venue\", \"Result\", \"GF\", \"GA\", \"Opponent\", \"Poss\"]\n",
    "nb_saisons = 6\n",
    "\n",
    "# En-tête de l'utilisateur pour ressembler à Mozilla Firefox\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'\n",
    "}\n",
    "\n",
    "# Fonction de contrôle du taux de requêtes\n",
    "MIN_REQUEST_INTERVAL = 2.5  # Interval minimum entre les requêtes (en secondes)\n",
    "last_request_time = None\n",
    "\n",
    "def rate_limit():\n",
    "    global last_request_time\n",
    "    if last_request_time is not None:\n",
    "        elapsed_time = time.time() - last_request_time\n",
    "        if elapsed_time < MIN_REQUEST_INTERVAL:\n",
    "            time.sleep(MIN_REQUEST_INTERVAL - elapsed_time)\n",
    "    last_request_time = time.time()\n",
    "\n",
    "df_final = []\n",
    "\n",
    "for i in range(nb_saisons):\n",
    "    rate_limit()  # Applique le contrôle du taux de requêtes\n",
    "\n",
    "    request1 = requests.get(url_ligue1, headers=headers)\n",
    "    soup = BeautifulSoup(request1.text, 'html.parser')\n",
    "    url_equipes = soup.select(\"table.stats_table\")[0].find_all(\"a\")\n",
    "    url_equipes = [equipes.get(\"href\") for equipes in url_equipes]\n",
    "    url_equipes = [equipes for equipes in url_equipes if equipes and \"squads\" in equipes]\n",
    "    url_equipes = [f\"https://fbref.com{i}\" for i in url_equipes]\n",
    "    url_ligue1 = f\"https://fbref.com{soup.find('a', class_='button2 prev').get('href')}\"\n",
    "\n",
    "    for j in range(len(url_equipes)):\n",
    "        rate_limit()  # Applique le contrôle du taux de requêtes\n",
    "\n",
    "        request2 = requests.get(url_equipes[j], headers=headers)\n",
    "        soup2 = BeautifulSoup(request2.text, 'html.parser')\n",
    "        df_equipe = pd.read_html(request2.text, match=\"Scores\")[0]\n",
    "\n",
    "        nom_equipe = url_equipes[j].split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        df_equipe[\"equipe\"] = nom_equipe\n",
    "\n",
    "        url_stats = soup2.findAll(\"a\")\n",
    "        url_stats = [el.get(\"href\") for el in url_stats]\n",
    "        url_stats = [el for el in url_stats if el and \"matchlogs/all_comps\" in el]\n",
    "        url_stats = [el for el in url_stats if el and (\"passing/\" in el or \"shooting\" in el or \"possession/\" in el or \"defense/\" in el or \"keeper\" in el)]\n",
    "        url_stats = list(set(url_stats))\n",
    "        url_stats = [f\"https://fbref.com{i}\" for i in url_stats]\n",
    "\n",
    "\n",
    "        for y in range(len(url_stats)):\n",
    "            rate_limit()  # Applique le contrôle du taux de requêtes\n",
    "\n",
    "            request3 = requests.get(url_stats[y], headers=headers)\n",
    "            soup3 = BeautifulSoup(request3.text, 'html.parser')\n",
    "            stats = pd.read_html(request3.text)[0]\n",
    "\n",
    "            if stats.columns.nlevels > 1:\n",
    "                stats.columns = [f\"{col}_{branch}\" if \"For\" not in col and \"Unnamed:\" not in col else f\"{branch}\" for col, branch in stats.columns]\n",
    "\n",
    "            stats.drop(left_on + [col for col in stats.columns if 'Report' in col], axis=1, inplace=True)\n",
    "\n",
    "            df_equipe = df_equipe.merge(stats, on=\"Date\")\n",
    "\n",
    "        df_final.append(df_equipe)\n",
    "\n",
    "df_final2 = pd.concat(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Scrapping.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m request1 \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url_ligue1, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(request1\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m url_equipes \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mtable.stats_table\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m url_equipes \u001b[39m=\u001b[39m [equipes\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m equipes \u001b[39min\u001b[39;00m url_equipes]\n\u001b[1;32m     <a href='vscode-notebook-cell://user-alexandreviard-610998-0.user.lab.sspcloud.fr/home/onyxia/work/Scrapping.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m url_equipes \u001b[39m=\u001b[39m [equipes \u001b[39mfor\u001b[39;00m equipes \u001b[39min\u001b[39;00m url_equipes \u001b[39mif\u001b[39;00m equipes \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msquads\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m equipes]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url_ligue1 = \"https://fbref.com/en/comps/13/Ligue-1-Stats\"\n",
    "left_on = [\"Time\", \"Comp\", \"Round\", \"Day\", \"Venue\", \"Result\", \"GF\", \"GA\", \"Opponent\"]\n",
    "df_final = []\n",
    "\n",
    "# En-tête de l'utilisateur pour ressembler à Mozilla Firefox\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'\n",
    "}\n",
    "\n",
    "# Fonction de contrôle du taux de requêtes\n",
    "MIN_REQUEST_INTERVAL = 2.5  # Interval minimum entre les requêtes (en secondes)\n",
    "last_request_time = None\n",
    "\n",
    "def rate_limit():\n",
    "    global last_request_time\n",
    "    if last_request_time is not None:\n",
    "        elapsed_time = time.time() - last_request_time\n",
    "        if elapsed_time < MIN_REQUEST_INTERVAL:\n",
    "            time.sleep(MIN_REQUEST_INTERVAL - elapsed_time)\n",
    "    last_request_time = time.time()\n",
    "\n",
    "request1 = requests.get(url_ligue1, headers=headers)\n",
    "soup = BeautifulSoup(request1.text, 'html.parser')\n",
    "url_equipes = soup.select(\"table.stats_table\")[0].find_all(\"a\")\n",
    "url_equipes = [equipes.get(\"href\") for equipes in url_equipes]\n",
    "url_equipes = [equipes for equipes in url_equipes if equipes and \"squads\" in equipes]\n",
    "url_equipes = [f\"https://fbref.com{i}\" for i in url_equipes]\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(url_equipes)):\n",
    "    rate_limit()  # Applique le contrôle du taux de requêtes\n",
    "\n",
    "    request2 = requests.get(url_equipes[j], headers=headers)\n",
    "    soup2 = BeautifulSoup(request2.text, 'html.parser')\n",
    "    df_equipe = pd.read_html(request2.text, match=\"Scores\")[0]\n",
    "\n",
    "    nom_equipe = url_equipes[j].split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    df_equipe[\"equipe\"] = nom_equipe\n",
    "\n",
    "    url_stats = soup2.findAll(\"a\")\n",
    "    url_stats = [el.get(\"href\") for el in url_stats]\n",
    "    url_stats = [el for el in url_stats if el and \"matchlogs/all_comps\" in el]\n",
    "    url_stats = [el for el in url_stats if el and (\"passing/\" in el or \"shooting\" in el or \"possession/\" in el or \"defense/\" in el or \"keeper\" in el)]\n",
    "    url_stats = list(set(url_stats))\n",
    "    url_stats = [f\"https://fbref.com{i}\" for i in url_stats]\n",
    "\n",
    "\n",
    "    for y in range(len(url_stats)):\n",
    "        rate_limit()  # Applique le contrôle du taux de requêtes\n",
    "\n",
    "        request3 = requests.get(url_stats[y], headers=headers)\n",
    "        soup3 = BeautifulSoup(request3.text, 'html.parser')\n",
    "        stats = pd.read_html(request3.text)[0]\n",
    "\n",
    "        if stats.columns.nlevels > 1:\n",
    "            stats.columns = [f\"{col}_{branch}\" if \"For\" not in col and \"Unnamed:\" not in col else f\"{branch}\" for col, branch in stats.columns]\n",
    "\n",
    "        stats.drop(left_on + [col for col in stats.columns if 'Report' in col], axis=1, inplace=True)\n",
    "\n",
    "        df_equipe = df_equipe.merge(stats, on=\"Date\")\n",
    "\n",
    "    df_final.append(df_equipe)\n",
    "\n",
    "df_final2 = pd.concat(df_final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
